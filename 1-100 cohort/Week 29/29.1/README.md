# Kubernetes Part 3 (Contd.)

- In this section, we will learn about:

  - [Volumes in Docker](#volumes-in-docker)
  - [Volumes in Kubernetes](#volumes-in-kubernetes)
  - [Creating a Ephemeral Volume in Kubernetes](#creating-a-ephemeral-volume-in-kubernetes)
  - [Creating a Persistent Volume in Kubernetes](#creating-a-persistent-volume-in-kubernetes)
  - [Automatic PV Creation](#automatic-pv-creation)
  - [Horizontal Pod Autoscaling](#horizontal-pod-autoscaling)

## Volumes in Docker

- Volumes are the preferred mechanism for persisting data generated by and used by Docker containers.

    <br>

- For eg:

  - If we have a docker container having a file 'randomdata.txt' and we write some data to this file, we would want to ensure that the data is not lost if the container crashes.
  - This is where volumes come in. Volumes are directories (or files) that are outside of the default Union File System and exist as normal directories and files on the host filesystem.
  - We can mount our container to a volume and write data to the volume. This way, even if the container crashes, the data is safe in the volume.

  <br>

### Binding Volumes

- To bind a volume to a container, we use the `-v` flag.

    <br>

- For eg: To bind the directory `/opt/datadir` on the host to the directory `/var/lib/mysql` in the container, we would use the following command:

  ```bash
  docker run -d --name mysql -v /opt/datadir:/var/lib/mysql mysql
  ```

  <br>

- This means that any data written to `/var/lib/mysql` in the container is actually written to `/opt/datadir` on the host. We can verify this by writing some data to `/var/lib/mysql` in the container and checking if the data is present in `/opt/datadir` on the host.

    <br>

### Volumes in Kubernetes

- In Kubernetes, we can also use volumes to persist data across container restarts.

    <br>

- Some Common Use cases:

    <br>

  1. **External Database Storage**: We can use volumes to persist database data even when container restarts.

    <br>

  2. **Pods sharing data**: Volumes can be used to share data between containers in a pod.

    <br>

  3. **Database inside a pod**: We can use volumes to persist data in a database running inside a pod.

    <br>

  4. **Pods needing extra storage**: We can use volumes to provide extra storage to pods during execution.

  <br>

- There are many types of volumes in Kubernetes. Some of the most common ones are:

    <br>

  1.  **Ephemeral Volume**: This volume is created when a pod is created and is deleted when the pod is deleted.

        <br>

      - _EmptyDir_: This volume is created when a pod is assigned to a node and is deleted when the pod is deleted. It is used to store temporary data.

        <br>

  2.  **Persistent Volume**: This volume is not deleted when the pod is deleted. It can be used by other pods as well.

    <br>

  3. **Persistent Volume Claim**: This is a request for storage by a pod. It is used to request storage from a Persistent Volume.

       <details>
       <summary><i><b>How it works?</b></i></summary>

      <img width="572" alt="Screenshot 2024-06-15 at 7 34 23 PM" src="https://github.com/its-id/100x-Cohort-Programs/assets/60315832/3905d56a-6a87-41e0-9abf-5ce161db3704">
      
     - When a pod is created, it requests storage from a Persistent Volume Claim.

     - The Persistent Volume Claim requests storage from a Persistent Volume.

     - The Persistent Volume provides the storage to the Persistent Volume Claim.

     - The Persistent Volume Claim provides the storage to the pod.

     - Usually, the **developer** only needs to create a Persistent Volume Claim. The Persistent Volume is created automatically by **DevOps Admin**.

       </details>

### Creating a Ephemeral Volume in Kubernetes

![image](https://github.com/user-attachments/assets/634fb947-e0e1-4c78-a803-8c2dbaa710e2)

1. Start with creating a cluster using the `ephemeral/cluster.yml` configuration:

   ```bash
   kind create cluster --config ephemeral/cluster.yml --name k8s-cluster
   ```

   <br>

2. Check the `ephemeral/deployment.yml` manifest file to check the configuration that starts two pods which write data to a shared volume. Create it using:

   ```bash
   kubectl apply -f ephemeral/deployment.yml
   ```

   <details>
    <summary><b>Explaining ephemeral/deployment.yml</b></summary>
    
    ```yaml
    apiVersion: apps/v1
    kind: Deployment
    metadata:
    name: shared-volume-deployment
    spec:
    replicas: 1
    selector:
        matchLabels:
        app: shared-volume-app
    template:
        metadata:
        labels:
            app: shared-volume-app
        spec:
        containers:
        - name: writer
            image: busybox
            command: ["/bin/sh", "-c", "echo 'Hello from Writer Pod' > /data/hello.txt; sleep 3600"]
            volumeMounts:
                - name: shared-data
                mountPath: /data
        - name: reader
            image: busybox
            command: ["/bin/sh", "-c", "cat /data/hello.txt; sleep 3600"]
            volumeMounts:
                - name: shared-data
                mountPath: /data
        volumes:
        - name: shared-data
            emptyDir: {}
   ```

   **Explanation**:

   - This manifest file creates a deployment with two pods: `writer` and `reader`.
   - `selector`: selects the pods with the label `app: shared-volume-app`.
   - `template`: contains the pod configuration.
     - `labels`: assigns the label `app: shared-volume-app` to the pod.
     - `containers`: contains the configuration for the two containers: `writer` and `reader`.
       - `writer`: writes data to the file `/data/hello.txt`.
       - `reader`: reads the data from the file `/data/hello.txt`.
     - `volumes`: creates a volume named `shared-data` of type `emptyDir`.
     - `volumeMounts`: mounts the volume `shared-data` to the directory `/data` in both the containers. If we start two replicas, then both the pods will have their own copy of the volume.

   </details>

   <br>

3. Check the `reader` container and see if it reads the data written by the `writer` container:

   ```bash
   kubectl exec -it shared-volume-deployment-<pod-id> --container reader sh
   ```

   <br>

4. You will notice that the `reader` container reads the data written by the `writer` container.

<br>

### Creating a Persistent Volume in Kubernetes

![image](https://github.com/user-attachments/assets/8cac512c-7b28-46b1-8e7c-87dab4d9c433)

1. Delete the previous cluster and restart a new cluster using the `persistent/cluster.yml` configuration:

   ```bash
   kind delete cluster --name k8s-cluster
   kind create cluster --config persistent/cluster.yml --name k8s-cluster
   ```

   <br>

2. We need to create a NFS (Networked File System) server to store the data. We can create one on cloud. For this demo, we will use a docker image to create an NFS server. Create the NFS server using the following command:

   ```bash
   kubectl apply -f persistent/nfs-server.yml
   ```

   <details>
   <summary><b>Explaining nfs-server.yml</b></summary>

   ```yaml
   version: '3.7'
   services:
     nfs-server:
       image: itsthenetwork/nfs-server-alpine:latest
       container_name: nfs-server
       privileged: true
       environment:
       SHARED_DIRECTORY: /exports
       volumes:
         - ./data:/exports:rw
       ports:
         - '2049:2049'
       restart: unless-stopped
   ```

   **Explanation**:

   - This manifest file creates an NFS server using the `itsthenetwork/nfs-server-alpine` image.

   - `nfs-server`: creates a service named `nfs-server`.

     - `image`: specifies the image to use for the NFS server.

       - `privileged`: gives the container full access to the host system.
       - `SHARED_DIRECTORY`: specifies the directory to share.

       - `volumes`: mounts the host directory `./data` to the container directory `/exports`.

       - `ports`: maps the port `2049` of the host to the port `2049` of the container.

       - `restart`: specifies the restart policy for the container.

   </details>

   <br>

3. Check the `persistent/pv.yml` manifest file to check the configuration that creates a Persistent Volume & Persistent Volume Claim. Create it using:

   ```bash
   kubectl apply -f persistent/pv.yml
   ```

   <details>
   <summary><b>Explaining pv.yml</b></summary>

   ```yaml
   apiVersion: v1
   kind: PersistentVolume
   metadata:
   name: nfs-pv
   spec:
   capacity:
     storage: 10Gi
   accessModes:
     - ReadWriteMany
   storageClassName: nfs
   nfs:
     path: /exports
     server: 52.66.197.168
   ---
   apiVersion: v1
   kind: PersistentVolumeClaim
   metadata:
   name: nfs-pvc
   spec:
   accessModes:
     - ReadWriteMany
   resources:
     requests:
     storage: 10Gi
   storageClassName: nfs
   ```

   **Explanation**:

   - This manifest file creates a Persistent Volume named `nfs-pv` and a Persistent Volume Claim named `nfs-pvc`.

   - `PersistentVolume`:

     - `capacity`: specifies the storage capacity of the volume i.e 10Gi.

     - `accessModes`: specifies the access mode for the volume i.e ReadWriteMany.

     - `storageClassName`: specifies the storage class name i.e nfs.

     - `nfs`: specifies the NFS server details i.e path and server IP.

       - `path`: specifies the path on the NFS server to store the data i.e /exports.

       - `server`: specifies the IP address of the NFS server. Here we use the IP address of the NFS server created in the previous step.

   - `PersistentVolumeClaim`:

     - `accessModes`: specifies the access mode for the volume i.e ReadWriteMany.

     - `resources`: specifies the storage capacity of the volume i.e 10Gi.

     - `storageClassName`: specifies the storage class name i.e nfs.

    </details>

    <br>

4. Now, to use the above volumes: Let's create a simple `mongodb` pod. Check the `persistent/mongodb.yml` manifest file to check the configuration that creates a pod with a Persistent Volume Claim. Create it using:

   ```bash
   kubectl apply -f persistent/mongodb.yml
   ```

   <details>
   <summary><b>Explaining mongodb.yml</b></summary>

   ```yaml
   apiVersion: v1
   kind: Pod
   metadata:
   name: mongo-pod
   spec:
   containers:
       - name: mongo
       image: mongo:4.4
       command: ['mongod', '--bind_ip_all']
       ports:
           - containerPort: 27017
       volumeMounts:
           - mountPath: '/data/db'
           name: nfs-volume
   volumes:
       - name: nfs-volume
       persistentVolumeClaim:
           claimName: nfs-pvc
   ```

   **Explanation**:

   - This manifest file creates a pod named `mongo-pod` with a container named `mongodb`.

   - `containers`:

   - `name`: specifies the name of the container i.e mongo-pod.

   - `image`: specifies the image to use for the container i.e mongo.

   - `volumeMounts`: mounts the volume `nfs-pv` to the directory `/data/db` in the container.

   - `volumes`:

     - `name`: specifies the name of the volume i.e nfs-pv.

     - `persistentVolumeClaim`: specifies the Persistent Volume Claim to use i.e nfs-pvc.

   </details>

   <br>

5. Try to connect to the `mongodb` pod and check if the data is persisted:

   ```bash
   kubectl exec -it mongo-pod --container mongo sh
   ```

   ```bash
   mongo
   ```

   ```bash
   show dbs
   ```

   ```bash
   use test
   ```

   ```bash
   db.test.insert({name: 'test'})
   ```

   ```bash
   db.test.find()
   ```

   <br>

6. Delete the pod and recreate it using the same Persistent Volume Claim:

   ```bash
    kubectl delete pod mongo-pod
    kubectl apply -f persistent/mongodb.yml
   ```

   <br>

7. Check if the data is still present:

   ```bash
    kubectl exec -it mongo-pod --container mongo sh
   ```

   ```bash
   mongo
   ```

   ```bash
   show dbs
   ```

   ```bash
   use test
   ```

   ```bash
   db.test.find()
   ```

   <br>

- You will notice that the data is persisted even if the pod is deleted.
  <img width="500" alt="Screenshot 2024-06-15 at 8 25 36 PM" src="https://github.com/its-id/100x-Cohort-Programs/assets/60315832/b5c7dcfc-4f41-45ce-b01b-ce9001da3ba3">


    <br>

### Automatic PV Creation

- In the previous example, we created the Persistent Volume manually.

    <br>

- **What if we just create the Persistent Volume Claim and let Kubernetes create the Persistent Volume automatically?**

    <br>

    <img width="500" alt="Screenshot 2024-06-15 at 8 11 59 PM" src="https://github.com/its-id/100x-Cohort-Programs/assets/60315832/1d37c764-5e16-4215-b405-413f1d41233b">

- If we use a cloud provider like AWS, GCP, or Azure, Kubernetes can automatically create the Persistent Volume for us.

- We can do this by specifying the `storageClassName` in the Persistent Volume Claim.

    <br>

1. Check the `persistent/pvc.yml` manifest file to check the configuration that creates a Persistent Volume Claim with a storage class. Create it using:

   ```bash
   kubectl apply -f automatic-pvc/pvc.yml
   ```

   <details>
   <summary><b>Explaining pvc.yml</b></summary>

   ```yaml
   apiVersion: v1
   kind: PersistentVolumeClaim
   metadata:
   name: csi-pvc
   spec:
   accessModes:
     - ReadWriteOnce
   resources:
     requests:
     storage: 40Gi
   storageClassName: vultr-block-storage-hdd
   ```

   **Explanation**:

   - This manifest file creates a Persistent Volume Claim named `csi-pvc` with a storage class named `vultr-block-storage-hdd`.

   - `accessModes`: specifies the access mode for the volume i.e ReadWriteMany.

   - `resources`: specifies the storage capacity of the volume i.e 40Gi.

   - `storageClassName`: specifies the storage class name i.e nfs.

   </details>

   <br>

- Here we are using the `vultr-block-storage-hdd` storage class. This storage class is provided by the Vultr CSI driver. You can check the storage classes available in your cluster using:

  ```bash
  kubectl get storageclass
  ```

    <br>

3.  Check the `persistent/mongodb-csi.yml` manifest file to check the configuration that creates a pod with a Persistent Volume Claim. Create it using:

    ```bash
    kubectl apply -f automatic-pvc/mongodb-csi.yml
    ```

    <details>
    <summary><b>Explaining mongodb-csi.yml</b></summary>

    ```yaml
    apiVersion: v1
    kind: Pod
    metadata:
    name: mongo-pod
    spec:
    containers:
    - name: mongo
        image: mongo:4.4
        command: ["mongod", "--bind_ip_all"]
        ports:
        - containerPort: 27017
        volumeMounts:
        - name: mongo-storage
        mountPath: /data/db
    volumes:
    - name: mongo-storage
        persistentVolumeClaim:
        claimName: csi-pvc
    ```

    **Explanation**:

    - This manifest file creates a pod named `mongo-pod` which uses the Persistent Volume Claim `csi-pvc`.

    - `containers`: contains the configuration for the container `mongo`.

      - `name`: specifies the name of the container i.e mongo-pod.

      - `command`: specifies the command to run in the container i.e `mongod --bind_ip_all` (starts the mongo server on all IP addresses).

        - `ports`: specifies the port to expose i.e 27017.

      - `image`: specifies the image to use for the container i.e mongo.

      - `volumeMounts`: mounts the volume `csi-pvc` to the directory `/data/db` in the container.

    - `volumes`:

      - `name`: specifies the name of the volume i.e csi-pvc.

        - `persistentVolumeClaim`: specifies the Persistent Volume Claim to use i.e csi-pvc.

    </details>

    <br>

4.  Check the status of the Persistent Volume Claim, Persistent Volume and pods using:

    ```bash
    kubectl get pvc
    kubectl get pv
    kubectl get pods
    ```

    <br>

5.  You will notice that the Persistent Volume Claim **is bound to a Persistent Volume** (created by Vultrr) and the pod is running.

    <br>

- This is how we can use Persistent Volumes in Kubernetes to persist data across container restarts.

---

## Horizontal Pod Autoscaling

- A Horizontal Pod Autoscaling is a Kubernetes feature that automatically scales the number of pods in a deployment based on observed CPU utilization.

    <br>

- Basically, it controls the `replicas` field in a deployment.

    <br>

- For eg:

  - If we have a deployment with **2 replicas** and the CPU utilization is high, the Horizontal Pod Autoscaler will increase the number of replicas **to 3**.

  - Similarly, if the CPU utilization is low, the Horizontal Pod Autoscaler will decrease the number of replicas to 1.

### Architecture

- The Horizontal Pod Autoscaler first gets the CPU utilization of the pods.

    <br>

- We can get the CPU utilization info of pods using external monitoring tools like [CAdvisor](https://github.com/google/cadvisor).

  <br>

- All of this info is stored in the `metrics-server`. Its not present by default in the cluster. We need to install it. You can install it using:

  ```bash
  kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
  ```

  **or** install it using the below github repo (since above installation is having some issues with Vultrr).

  ```bash
  kubectl apply -f https://github.com/100xdevs-cohort-2/week-28-manifests
  ```

  <br>

- Try getting the CPU metrics using:

  ```bash
  kubectl top pod -n kube-system
  kubectl top nodes -n kube-system
  ```

  <br>

- The Horizontal Pod Autoscaler then gets the desired CPU utilization from the worker nodes which is then passed to the `hpa-controller` inside the Master Node via `API Server` which then scales the number of replicas in the deployment.

    <br>

- In summary, the overall flow looks like this:
  <img width="600" alt="Screenshot 2024-06-15 at 8 11 59 PM" src="https://github.com/its-id/100x-Cohort-Programs/assets/60315832/9a1743df-f890-4605-858d-1c2995ec5105">
